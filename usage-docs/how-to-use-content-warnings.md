# All About Content Warnings

The short version: content warnings are a feature that allows you to
obscure your content in such a way that it is hidden by default in other
users' timelines. Instead, only the text of the content warning is
displayed. To put it another way, if you were to put a content warning on
one of your posts that read "descriptions of war violence" while discussing
current or past wars, users would only see that description and could then
choose to click through the content warning to view the content (or not).

## How to Apply a Content Warning

In order to apply a content warning use the "CW" in the post field:

<img src="../assets/mastodon-content-warning-button.png" 
     width="400"
     alt="Screenshot of the post field with the CW circled and with
          an arrow pointing to it" />

An example post with a content warning on the text and image looks like
this:

<img src="../assets/mastodon-content-warning.png"
     alt="Screenshot of a post with content warning 'Politics (CO)' and the
          blurred out content labeled 'Sensitive Content'"
     width="400"/>

## When to Use Content Warnings

Since the content warning feature is a bit different than Twitter, there
can be confusion about when to use them. To help,
here are some general guidelines.

* A content warning should be used to protect the psychological safety of
  others in a responsible way
* Spoilers

The latter is quickst to explain, so let's start there.

### "Spoilers, Sweetie"

(Quote attributed to River Song of Doctor Who fame.)

Preventing the spread of spoilers is an excellent "off label" use of the
content warning feature. Spoilers most commonly refers to current books,
television, and movies. One quick, easy example would be using a content
warning when discussing Game of Thrones when it was actively airing.
Another would be discussing the results of a football / soccer match that
people maybe haven't been able to catch up on yet. Different fandoms have
different expectations for how long information is new enough to be
"spoiler" worthy, so this is "at user discretion" and not something
we try to enforce as mods. Just treat other fans the way you would
want to be treated.

### Protecting Psychological Safety

This one will take more time and effort on your part as users as it takes
only a moment to explain but takes growth and improving over time to
manage effectively. The short version is:

> **You should use a content warning whenever the psychologically safest
option is to opt into a conversation rather than to default into that
conversation.**

I'm going to start with a hopefully clear example. Let's say that you're
focusing on human rights abuses around the world. You are in a position
to also directly share, or share news stores that display, images of the
abuses that are happening to show others the reality of that situation.
This could include a lot of violent and traumatic video and images. Due to
the nature of the situation(s), you may want to make sure that people can
see the content to avoid turning a blind eye.

That said: you cannot control the reach of your information.
This means that by not using a content warning, not only will the people
you wish would pay attention potentially see it (or not, depending on how
their home instance federates), but you could also be exposing victims
of that same violence to content that triggers their trauma. So what's the
psychologically safest option? To protect the most vulnerable, in this case
the people with trauma, and that means use a content warning. The post will
still have the same reach, but it allows people to opt into that conversation.

Similar cases to the above would be any posts that includes text
descriptions of, or images and video of, violent actions including but not
limited to physical or sexual violence. To be clear: our definition of
violence is not limited to being between humans, but includes animal
abuse as well.

Areas that might take more learning and growth to understand and adopt
are normally those that involve understanding intersectionality of users
on the platform. For example, you may see people post content warning on
images with faces (especially with eye contact) or over food. With these
specific examples using a content warning for faces and eye contact is to
help neurodiverse users on the platform who struggle with these and have
strong adverse reactions to it, and for the latter it can be to help those with eating disorders or who are recovering. (Or similar for images of
alcohol for users who struggle / are recovering from alcohol addiction.)

For taking an active part in keeping Hachyderm a psychologically safe
place to be, it is important to do the work of understanding others
and apply that knowledge to how you interact with each other. There is
a lot to be said here, and it would exceed this document's scope, but a good place
to start is learning about anti-racism, accessibility, and anti-ableism.

## What Do the Moderators Enforce

Succinctly:

* Moderators will not take moderation action against spoilers, but
  it really is in poor form to openly share spoilers.
* Moderators will protect the psychological safety of users and prioritize
  the most vulnerable.
  
For the latter: this means we will handle reports that violate the
psychological safety of others equally whether they occurred on our
instance or by one of our users interacting with users on other instances.
To rephrase with a concrete example, we'll treat a report about posting violent images the same whether someone is posting it on their timeline or
posting it to another user regardless of whether or not that other user is a
Hachydermian.

### What to Include in a Report

Remember our moderators are human and will be doing our best to
ensure that all users feel safe on our platform. If you are ever in a
position where you need to file a report against a user for not using
content warnings when they should, please make sure to help us help you
by:

* Including the post(s) in the report, so we don't need to search for them
* Including information for why the content is problematic, even if it
  seems obvious.
  
There are far more of you than us, and these two things will help us
handle your reports effectively and efficiently.
